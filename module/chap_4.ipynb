{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22f81222",
   "metadata": {},
   "source": [
    "# **4 Implementing a GPT model from scratch to generate text**\n",
    "\n",
    "## **4.1 Coding an LLM architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "477f1bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "142c8073",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16942582",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyTransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This block does nothing and just returns its input.\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        # The parameters here are just to mimic the LayerNorm interface.\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16b46485",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        # Use a placeholder for TransformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg['n_layers'])]\n",
    "            )\n",
    "        # Use a placeholder for LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg['emb_dim'], cfg[\"vocab_size\"], bias = False\n",
    "        )\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06d93a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6109, 3626, 6100,  345],\n",
       "        [6109, 1110, 6622,  257]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "batch = []\n",
    "\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "\n",
    "batch = torch.stack(batch, dim=0)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c78595f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape:  torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "logits = model(batch)\n",
    "print(\"Output Shape: \", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f2ef73",
   "metadata": {},
   "source": [
    "## **4.2 Normalizing activations with layer normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e3c18b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3374, -0.1778, -0.3035,  ..., -1.5753, -0.6813, -0.4181],\n",
       "        [ 0.2573,  0.6937,  0.4207,  ..., -0.3439,  0.0665, -0.1417],\n",
       "        [ 2.5754, -1.3320,  0.2282,  ...,  0.1268,  2.6668, -1.3954],\n",
       "        ...,\n",
       "        [-0.7404,  1.6838, -0.7466,  ...,  1.4978,  1.5824,  0.1382],\n",
       "        [-0.4052, -0.3417, -1.2393,  ..., -0.8622,  0.3899, -2.1674],\n",
       "        [-0.3101,  0.2659,  0.2810,  ...,  1.5808, -1.4378,  2.6974]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "batch_example = torch.randn(1000,1000)\n",
    "batch_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19a1a703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.4110, 0.0000, 0.4312],\n",
       "        [0.4000, 1.2636, 0.6592,  ..., 0.1944, 0.4920, 0.7376],\n",
       "        [0.2260, 0.0000, 0.0310,  ..., 0.0000, 0.0000, 0.7800],\n",
       "        ...,\n",
       "        [0.5723, 0.2858, 0.9167,  ..., 0.0000, 1.0340, 0.7567],\n",
       "        [0.4171, 0.4055, 0.0000,  ..., 0.6105, 0.1221, 0.0000],\n",
       "        [0.0000, 0.5360, 0.8316,  ..., 0.0139, 0.8438, 0.1777]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = nn.Sequential(nn.Linear(1000,10), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f096e4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1745],\n",
       "        [0.3747],\n",
       "        [0.1395],\n",
       "        [0.3079],\n",
       "        [0.1360],\n",
       "        [0.2788],\n",
       "        [0.1580],\n",
       "        [0.2092],\n",
       "        [0.0935],\n",
       "        [0.1467],\n",
       "        [0.2579],\n",
       "        [0.2674],\n",
       "        [0.2814],\n",
       "        [0.2149],\n",
       "        [0.2921],\n",
       "        [0.1800],\n",
       "        [0.3233],\n",
       "        [0.1616],\n",
       "        [0.3239],\n",
       "        [0.3288],\n",
       "        [0.4210],\n",
       "        [0.1416],\n",
       "        [0.1825],\n",
       "        [0.1306],\n",
       "        [0.2717],\n",
       "        [0.4673],\n",
       "        [0.3418],\n",
       "        [0.1229],\n",
       "        [0.1214],\n",
       "        [0.1386],\n",
       "        [0.3089],\n",
       "        [0.1729],\n",
       "        [0.1103],\n",
       "        [0.2573],\n",
       "        [0.1715],\n",
       "        [0.1455],\n",
       "        [0.1742],\n",
       "        [0.2827],\n",
       "        [0.2615],\n",
       "        [0.1644],\n",
       "        [0.1953],\n",
       "        [0.3117],\n",
       "        [0.2878],\n",
       "        [0.0955],\n",
       "        [0.1235],\n",
       "        [0.1653],\n",
       "        [0.4099],\n",
       "        [0.3649],\n",
       "        [0.2076],\n",
       "        [0.1700],\n",
       "        [0.2151],\n",
       "        [0.5239],\n",
       "        [0.4395],\n",
       "        [0.0214],\n",
       "        [0.2322],\n",
       "        [0.1083],\n",
       "        [0.2580],\n",
       "        [0.4414],\n",
       "        [0.0320],\n",
       "        [0.1560],\n",
       "        [0.2525],\n",
       "        [0.3107],\n",
       "        [0.1060],\n",
       "        [0.1486],\n",
       "        [0.2768],\n",
       "        [0.2258],\n",
       "        [0.3531],\n",
       "        [0.2339],\n",
       "        [0.2473],\n",
       "        [0.1175],\n",
       "        [0.2518],\n",
       "        [0.1909],\n",
       "        [0.1730],\n",
       "        [0.1561],\n",
       "        [0.3743],\n",
       "        [0.2369],\n",
       "        [0.2152],\n",
       "        [0.0775],\n",
       "        [0.4086],\n",
       "        [0.1913],\n",
       "        [0.4097],\n",
       "        [0.1581],\n",
       "        [0.1483],\n",
       "        [0.2462],\n",
       "        [0.1804],\n",
       "        [0.1295],\n",
       "        [0.0713],\n",
       "        [0.3839],\n",
       "        [0.1003],\n",
       "        [0.2880],\n",
       "        [0.0850],\n",
       "        [0.3699],\n",
       "        [0.2779],\n",
       "        [0.1412],\n",
       "        [0.3079],\n",
       "        [0.0887],\n",
       "        [0.1524],\n",
       "        [0.2610],\n",
       "        [0.4193],\n",
       "        [0.1437],\n",
       "        [0.1512],\n",
       "        [0.1088],\n",
       "        [0.1994],\n",
       "        [0.0818],\n",
       "        [0.2241],\n",
       "        [0.3285],\n",
       "        [0.0628],\n",
       "        [0.2315],\n",
       "        [0.1150],\n",
       "        [0.2088],\n",
       "        [0.2843],\n",
       "        [0.0436],\n",
       "        [0.2439],\n",
       "        [0.0660],\n",
       "        [0.1859],\n",
       "        [0.1848],\n",
       "        [0.2643],\n",
       "        [0.2719],\n",
       "        [0.2108],\n",
       "        [0.0439],\n",
       "        [0.2111],\n",
       "        [0.0411],\n",
       "        [0.2439],\n",
       "        [0.2489],\n",
       "        [0.3013],\n",
       "        [0.1437],\n",
       "        [0.2092],\n",
       "        [0.1789],\n",
       "        [0.1996],\n",
       "        [0.0227],\n",
       "        [0.1629],\n",
       "        [0.1797],\n",
       "        [0.2751],\n",
       "        [0.3759],\n",
       "        [0.1313],\n",
       "        [0.2414],\n",
       "        [0.0487],\n",
       "        [0.2108],\n",
       "        [0.1834],\n",
       "        [0.1299],\n",
       "        [0.1153],\n",
       "        [0.5089],\n",
       "        [0.2836],\n",
       "        [0.1157],\n",
       "        [0.1590],\n",
       "        [0.3287],\n",
       "        [0.2434],\n",
       "        [0.1356],\n",
       "        [0.2865],\n",
       "        [0.5334],\n",
       "        [0.2181],\n",
       "        [0.3850],\n",
       "        [0.6254],\n",
       "        [0.0796],\n",
       "        [0.3359],\n",
       "        [0.3241],\n",
       "        [0.0758],\n",
       "        [0.1108],\n",
       "        [0.2904],\n",
       "        [0.0712],\n",
       "        [0.3448],\n",
       "        [0.2266],\n",
       "        [0.2277],\n",
       "        [0.2266],\n",
       "        [0.1270],\n",
       "        [0.0995],\n",
       "        [0.0755],\n",
       "        [0.3031],\n",
       "        [0.1716],\n",
       "        [0.1332],\n",
       "        [0.1033],\n",
       "        [0.5405],\n",
       "        [0.3500],\n",
       "        [0.2770],\n",
       "        [0.0492],\n",
       "        [0.0969],\n",
       "        [0.2850],\n",
       "        [0.0968],\n",
       "        [0.1818],\n",
       "        [0.1143],\n",
       "        [0.2149],\n",
       "        [0.1703],\n",
       "        [0.1933],\n",
       "        [0.1595],\n",
       "        [0.2434],\n",
       "        [0.0629],\n",
       "        [0.3279],\n",
       "        [0.0122],\n",
       "        [0.3447],\n",
       "        [0.2316],\n",
       "        [0.2166],\n",
       "        [0.0123],\n",
       "        [0.2045],\n",
       "        [0.1165],\n",
       "        [0.1841],\n",
       "        [0.2545],\n",
       "        [0.1274],\n",
       "        [0.2822],\n",
       "        [0.3545],\n",
       "        [0.4233],\n",
       "        [0.3275],\n",
       "        [0.1606],\n",
       "        [0.1069],\n",
       "        [0.2324],\n",
       "        [0.3198],\n",
       "        [0.0580],\n",
       "        [0.1497],\n",
       "        [0.1779],\n",
       "        [0.1792],\n",
       "        [0.3504],\n",
       "        [0.2381],\n",
       "        [0.2691],\n",
       "        [0.1196],\n",
       "        [0.2088],\n",
       "        [0.0819],\n",
       "        [0.3176],\n",
       "        [0.3024],\n",
       "        [0.1789],\n",
       "        [0.3102],\n",
       "        [0.3118],\n",
       "        [0.2851],\n",
       "        [0.1713],\n",
       "        [0.2499],\n",
       "        [0.2217],\n",
       "        [0.2080],\n",
       "        [0.3924],\n",
       "        [0.0753],\n",
       "        [0.2977],\n",
       "        [0.1313],\n",
       "        [0.2495],\n",
       "        [0.3465],\n",
       "        [0.2755],\n",
       "        [0.3919],\n",
       "        [0.0678],\n",
       "        [0.4614],\n",
       "        [0.1225],\n",
       "        [0.1580],\n",
       "        [0.1940],\n",
       "        [0.3415],\n",
       "        [0.1588],\n",
       "        [0.2254],\n",
       "        [0.2046],\n",
       "        [0.2800],\n",
       "        [0.3273],\n",
       "        [0.3850],\n",
       "        [0.2900],\n",
       "        [0.2243],\n",
       "        [0.1716],\n",
       "        [0.4117],\n",
       "        [0.2325],\n",
       "        [0.2035],\n",
       "        [0.2186],\n",
       "        [0.1371],\n",
       "        [0.3170],\n",
       "        [0.2224],\n",
       "        [0.1638],\n",
       "        [0.5851],\n",
       "        [0.1055],\n",
       "        [0.3544],\n",
       "        [0.2997],\n",
       "        [0.1985],\n",
       "        [0.1030],\n",
       "        [0.3961],\n",
       "        [0.2277],\n",
       "        [0.0918],\n",
       "        [0.0643],\n",
       "        [0.4113],\n",
       "        [0.2168],\n",
       "        [0.1100],\n",
       "        [0.3353],\n",
       "        [0.1998],\n",
       "        [0.1088],\n",
       "        [0.1895],\n",
       "        [0.0675],\n",
       "        [0.2015],\n",
       "        [0.1626],\n",
       "        [0.3144],\n",
       "        [0.1774],\n",
       "        [0.0553],\n",
       "        [0.2107],\n",
       "        [0.1312],\n",
       "        [0.2060],\n",
       "        [0.1988],\n",
       "        [0.4097],\n",
       "        [0.2591],\n",
       "        [0.2914],\n",
       "        [0.1040],\n",
       "        [0.3994],\n",
       "        [0.1524],\n",
       "        [0.0879],\n",
       "        [0.4675],\n",
       "        [0.2370],\n",
       "        [0.2571],\n",
       "        [0.2190],\n",
       "        [0.2165],\n",
       "        [0.3368],\n",
       "        [0.2048],\n",
       "        [0.4525],\n",
       "        [0.3747],\n",
       "        [0.0633],\n",
       "        [0.2230],\n",
       "        [0.0976],\n",
       "        [0.0613],\n",
       "        [0.1686],\n",
       "        [0.2130],\n",
       "        [0.4464],\n",
       "        [0.2389],\n",
       "        [0.0622],\n",
       "        [0.1327],\n",
       "        [0.3586],\n",
       "        [0.1313],\n",
       "        [0.2015],\n",
       "        [0.1567],\n",
       "        [0.3690],\n",
       "        [0.3594],\n",
       "        [0.3022],\n",
       "        [0.1838],\n",
       "        [0.2690],\n",
       "        [0.1176],\n",
       "        [0.1037],\n",
       "        [0.3386],\n",
       "        [0.4514],\n",
       "        [0.1274],\n",
       "        [0.3223],\n",
       "        [0.2887],\n",
       "        [0.0795],\n",
       "        [0.1073],\n",
       "        [0.3388],\n",
       "        [0.0847],\n",
       "        [0.2310],\n",
       "        [0.2643],\n",
       "        [0.2134],\n",
       "        [0.2931],\n",
       "        [0.2468],\n",
       "        [0.2446],\n",
       "        [0.1528],\n",
       "        [0.2540],\n",
       "        [0.1782],\n",
       "        [0.0469],\n",
       "        [0.1872],\n",
       "        [0.3034],\n",
       "        [0.2855],\n",
       "        [0.2174],\n",
       "        [0.1647],\n",
       "        [0.2687],\n",
       "        [0.2085],\n",
       "        [0.2127],\n",
       "        [0.1680],\n",
       "        [0.2378],\n",
       "        [0.0960],\n",
       "        [0.3460],\n",
       "        [0.2240],\n",
       "        [0.3618],\n",
       "        [0.1158],\n",
       "        [0.2170],\n",
       "        [0.1577],\n",
       "        [0.5011],\n",
       "        [0.1662],\n",
       "        [0.3463],\n",
       "        [0.2283],\n",
       "        [0.1023],\n",
       "        [0.1754],\n",
       "        [0.0000],\n",
       "        [0.3283],\n",
       "        [0.1810],\n",
       "        [0.2965],\n",
       "        [0.2875],\n",
       "        [0.2203],\n",
       "        [0.3206],\n",
       "        [0.2700],\n",
       "        [0.1963],\n",
       "        [0.2078],\n",
       "        [0.1357],\n",
       "        [0.1881],\n",
       "        [0.2508],\n",
       "        [0.2443],\n",
       "        [0.2645],\n",
       "        [0.2396],\n",
       "        [0.1181],\n",
       "        [0.2579],\n",
       "        [0.1239],\n",
       "        [0.1412],\n",
       "        [0.1990],\n",
       "        [0.1984],\n",
       "        [0.1159],\n",
       "        [0.1915],\n",
       "        [0.4790],\n",
       "        [0.1433],\n",
       "        [0.0688],\n",
       "        [0.1612],\n",
       "        [0.0822],\n",
       "        [0.3418],\n",
       "        [0.3294],\n",
       "        [0.1836],\n",
       "        [0.3555],\n",
       "        [0.2953],\n",
       "        [0.3786],\n",
       "        [0.2948],\n",
       "        [0.2523],\n",
       "        [0.1884],\n",
       "        [0.1472],\n",
       "        [0.1589],\n",
       "        [0.2452],\n",
       "        [0.3613],\n",
       "        [0.2996],\n",
       "        [0.5186],\n",
       "        [0.0513],\n",
       "        [0.2099],\n",
       "        [0.1923],\n",
       "        [0.2083],\n",
       "        [0.2657],\n",
       "        [0.2050],\n",
       "        [0.1229],\n",
       "        [0.2676],\n",
       "        [0.1353],\n",
       "        [0.3173],\n",
       "        [0.2522],\n",
       "        [0.3503],\n",
       "        [0.2070],\n",
       "        [0.1964],\n",
       "        [0.2106],\n",
       "        [0.1741],\n",
       "        [0.1692],\n",
       "        [0.3526],\n",
       "        [0.1496],\n",
       "        [0.1042],\n",
       "        [0.2151],\n",
       "        [0.3398],\n",
       "        [0.0938],\n",
       "        [0.1635],\n",
       "        [0.1450],\n",
       "        [0.4848],\n",
       "        [0.2135],\n",
       "        [0.2684],\n",
       "        [0.4432],\n",
       "        [0.2517],\n",
       "        [0.2631],\n",
       "        [0.3268],\n",
       "        [0.1850],\n",
       "        [0.3065],\n",
       "        [0.3267],\n",
       "        [0.1287],\n",
       "        [0.1634],\n",
       "        [0.3315],\n",
       "        [0.1564],\n",
       "        [0.1249],\n",
       "        [0.1281],\n",
       "        [0.3812],\n",
       "        [0.1854],\n",
       "        [0.3084],\n",
       "        [0.1314],\n",
       "        [0.3139],\n",
       "        [0.4420],\n",
       "        [0.3003],\n",
       "        [0.1902],\n",
       "        [0.2163],\n",
       "        [0.1163],\n",
       "        [0.2122],\n",
       "        [0.2147],\n",
       "        [0.1605],\n",
       "        [0.1936],\n",
       "        [0.3036],\n",
       "        [0.2462],\n",
       "        [0.4713],\n",
       "        [0.1024],\n",
       "        [0.3189],\n",
       "        [0.1426],\n",
       "        [0.3181],\n",
       "        [0.1595],\n",
       "        [0.1552],\n",
       "        [0.2847],\n",
       "        [0.1064],\n",
       "        [0.5183],\n",
       "        [0.2660],\n",
       "        [0.1243],\n",
       "        [0.4654],\n",
       "        [0.1815],\n",
       "        [0.2471],\n",
       "        [0.1545],\n",
       "        [0.0358],\n",
       "        [0.3174],\n",
       "        [0.3041],\n",
       "        [0.1476],\n",
       "        [0.1789],\n",
       "        [0.2354],\n",
       "        [0.4406],\n",
       "        [0.2457],\n",
       "        [0.3205],\n",
       "        [0.1698],\n",
       "        [0.3301],\n",
       "        [0.1087],\n",
       "        [0.2984],\n",
       "        [0.1701],\n",
       "        [0.1883],\n",
       "        [0.2856],\n",
       "        [0.3214],\n",
       "        [0.1344],\n",
       "        [0.1908],\n",
       "        [0.1605],\n",
       "        [0.2978],\n",
       "        [0.1043],\n",
       "        [0.1660],\n",
       "        [0.3929],\n",
       "        [0.3286],\n",
       "        [0.1940],\n",
       "        [0.2608],\n",
       "        [0.3323],\n",
       "        [0.4294],\n",
       "        [0.1896],\n",
       "        [0.1173],\n",
       "        [0.2334],\n",
       "        [0.1194],\n",
       "        [0.0959],\n",
       "        [0.3346],\n",
       "        [0.1805],\n",
       "        [0.4150],\n",
       "        [0.2149],\n",
       "        [0.2977],\n",
       "        [0.1658],\n",
       "        [0.2768],\n",
       "        [0.1980],\n",
       "        [0.0645],\n",
       "        [0.1391],\n",
       "        [0.1092],\n",
       "        [0.3061],\n",
       "        [0.1585],\n",
       "        [0.3626],\n",
       "        [0.3053],\n",
       "        [0.2743],\n",
       "        [0.1505],\n",
       "        [0.2350],\n",
       "        [0.4222],\n",
       "        [0.1554],\n",
       "        [0.2679],\n",
       "        [0.2142],\n",
       "        [0.2274],\n",
       "        [0.0879],\n",
       "        [0.2427],\n",
       "        [0.1860],\n",
       "        [0.3129],\n",
       "        [0.1556],\n",
       "        [0.2318],\n",
       "        [0.1881],\n",
       "        [0.0372],\n",
       "        [0.1723],\n",
       "        [0.2340],\n",
       "        [0.1607],\n",
       "        [0.3043],\n",
       "        [0.3226],\n",
       "        [0.1564],\n",
       "        [0.1285],\n",
       "        [0.1904],\n",
       "        [0.1546],\n",
       "        [0.3502],\n",
       "        [0.3125],\n",
       "        [0.0441],\n",
       "        [0.0726],\n",
       "        [0.1022],\n",
       "        [0.2338],\n",
       "        [0.3750],\n",
       "        [0.0797],\n",
       "        [0.2069],\n",
       "        [0.3533],\n",
       "        [0.1099],\n",
       "        [0.1951],\n",
       "        [0.3055],\n",
       "        [0.1148],\n",
       "        [0.1786],\n",
       "        [0.2982],\n",
       "        [0.3022],\n",
       "        [0.3472],\n",
       "        [0.3696],\n",
       "        [0.3574],\n",
       "        [0.3724],\n",
       "        [0.1971],\n",
       "        [0.2143],\n",
       "        [0.2105],\n",
       "        [0.0903],\n",
       "        [0.3254],\n",
       "        [0.1541],\n",
       "        [0.1390],\n",
       "        [0.1787],\n",
       "        [0.0877],\n",
       "        [0.2848],\n",
       "        [0.2192],\n",
       "        [0.1473],\n",
       "        [0.2388],\n",
       "        [0.4064],\n",
       "        [0.2004],\n",
       "        [0.2471],\n",
       "        [0.1694],\n",
       "        [0.4666],\n",
       "        [0.2677],\n",
       "        [0.2607],\n",
       "        [0.2679],\n",
       "        [0.1816],\n",
       "        [0.2216],\n",
       "        [0.2902],\n",
       "        [0.1229],\n",
       "        [0.1429],\n",
       "        [0.1219],\n",
       "        [0.4243],\n",
       "        [0.1542],\n",
       "        [0.2266],\n",
       "        [0.2984],\n",
       "        [0.2466],\n",
       "        [0.1679],\n",
       "        [0.2803],\n",
       "        [0.3971],\n",
       "        [0.2737],\n",
       "        [0.1735],\n",
       "        [0.0943],\n",
       "        [0.2407],\n",
       "        [0.0641],\n",
       "        [0.1475],\n",
       "        [0.0220],\n",
       "        [0.2430],\n",
       "        [0.0785],\n",
       "        [0.2934],\n",
       "        [0.0447],\n",
       "        [0.1393],\n",
       "        [0.1634],\n",
       "        [0.2035],\n",
       "        [0.2147],\n",
       "        [0.1732],\n",
       "        [0.2922],\n",
       "        [0.2673],\n",
       "        [0.2137],\n",
       "        [0.1773],\n",
       "        [0.1859],\n",
       "        [0.1541],\n",
       "        [0.1106],\n",
       "        [0.3301],\n",
       "        [0.1807],\n",
       "        [0.2151],\n",
       "        [0.1048],\n",
       "        [0.3228],\n",
       "        [0.2589],\n",
       "        [0.2084],\n",
       "        [0.2135],\n",
       "        [0.1932],\n",
       "        [0.2214],\n",
       "        [0.3628],\n",
       "        [0.1088],\n",
       "        [0.2861],\n",
       "        [0.2142],\n",
       "        [0.2566],\n",
       "        [0.1474],\n",
       "        [0.3273],\n",
       "        [0.0952],\n",
       "        [0.3590],\n",
       "        [0.1097],\n",
       "        [0.3115],\n",
       "        [0.3448],\n",
       "        [0.2402],\n",
       "        [0.1761],\n",
       "        [0.1345],\n",
       "        [0.6401],\n",
       "        [0.1682],\n",
       "        [0.4347],\n",
       "        [0.1885],\n",
       "        [0.1731],\n",
       "        [0.3695],\n",
       "        [0.1434],\n",
       "        [0.1777],\n",
       "        [0.3182],\n",
       "        [0.0523],\n",
       "        [0.2473],\n",
       "        [0.2104],\n",
       "        [0.1544],\n",
       "        [0.2278],\n",
       "        [0.2209],\n",
       "        [0.1657],\n",
       "        [0.1712],\n",
       "        [0.2516],\n",
       "        [0.2612],\n",
       "        [0.1767],\n",
       "        [0.1590],\n",
       "        [0.3974],\n",
       "        [0.1938],\n",
       "        [0.0676],\n",
       "        [0.2424],\n",
       "        [0.1761],\n",
       "        [0.1833],\n",
       "        [0.2620],\n",
       "        [0.3749],\n",
       "        [0.3953],\n",
       "        [0.2296],\n",
       "        [0.3334],\n",
       "        [0.1434],\n",
       "        [0.2297],\n",
       "        [0.0760],\n",
       "        [0.1577],\n",
       "        [0.2548],\n",
       "        [0.1419],\n",
       "        [0.1340],\n",
       "        [0.1118],\n",
       "        [0.1681],\n",
       "        [0.2182],\n",
       "        [0.2472],\n",
       "        [0.4208],\n",
       "        [0.2961],\n",
       "        [0.1879],\n",
       "        [0.1751],\n",
       "        [0.2865],\n",
       "        [0.2279],\n",
       "        [0.1769],\n",
       "        [0.0626],\n",
       "        [0.2286],\n",
       "        [0.1097],\n",
       "        [0.0990],\n",
       "        [0.0933],\n",
       "        [0.2081],\n",
       "        [0.1984],\n",
       "        [0.1902],\n",
       "        [0.2836],\n",
       "        [0.1451],\n",
       "        [0.3703],\n",
       "        [0.2462],\n",
       "        [0.1801],\n",
       "        [0.2611],\n",
       "        [0.2330],\n",
       "        [0.2723],\n",
       "        [0.1262],\n",
       "        [0.3295],\n",
       "        [0.3393],\n",
       "        [0.1564],\n",
       "        [0.3462],\n",
       "        [0.2965],\n",
       "        [0.2892],\n",
       "        [0.2266],\n",
       "        [0.2662],\n",
       "        [0.2696],\n",
       "        [0.1476],\n",
       "        [0.2277],\n",
       "        [0.0406],\n",
       "        [0.1047],\n",
       "        [0.1300],\n",
       "        [0.0477],\n",
       "        [0.3439],\n",
       "        [0.0680],\n",
       "        [0.2186],\n",
       "        [0.1482],\n",
       "        [0.1977],\n",
       "        [0.1933],\n",
       "        [0.1435],\n",
       "        [0.3882],\n",
       "        [0.1463],\n",
       "        [0.2113],\n",
       "        [0.1491],\n",
       "        [0.3865],\n",
       "        [0.2476],\n",
       "        [0.2134],\n",
       "        [0.2263],\n",
       "        [0.2305],\n",
       "        [0.1455],\n",
       "        [0.3506],\n",
       "        [0.3541],\n",
       "        [0.1057],\n",
       "        [0.2124],\n",
       "        [0.3030],\n",
       "        [0.1080],\n",
       "        [0.1694],\n",
       "        [0.2086],\n",
       "        [0.1488],\n",
       "        [0.2077],\n",
       "        [0.3021],\n",
       "        [0.2389],\n",
       "        [0.1740],\n",
       "        [0.3381],\n",
       "        [0.0675],\n",
       "        [0.3374],\n",
       "        [0.1788],\n",
       "        [0.0656],\n",
       "        [0.0578],\n",
       "        [0.1774],\n",
       "        [0.2472],\n",
       "        [0.0230],\n",
       "        [0.2403],\n",
       "        [0.1538],\n",
       "        [0.1346],\n",
       "        [0.0626],\n",
       "        [0.4900],\n",
       "        [0.3201],\n",
       "        [0.2558],\n",
       "        [0.1351],\n",
       "        [0.2119],\n",
       "        [0.3006],\n",
       "        [0.2115],\n",
       "        [0.2285],\n",
       "        [0.2154],\n",
       "        [0.4320],\n",
       "        [0.2568],\n",
       "        [0.1152],\n",
       "        [0.1775],\n",
       "        [0.1768],\n",
       "        [0.1040],\n",
       "        [0.3381],\n",
       "        [0.2217],\n",
       "        [0.1521],\n",
       "        [0.0694],\n",
       "        [0.1342],\n",
       "        [0.1409],\n",
       "        [0.2806],\n",
       "        [0.2285],\n",
       "        [0.2180],\n",
       "        [0.4330],\n",
       "        [0.0479],\n",
       "        [0.2208],\n",
       "        [0.1240],\n",
       "        [0.2710],\n",
       "        [0.3378],\n",
       "        [0.3764],\n",
       "        [0.4015],\n",
       "        [0.4706],\n",
       "        [0.0844],\n",
       "        [0.2483],\n",
       "        [0.1832],\n",
       "        [0.2470],\n",
       "        [0.3189],\n",
       "        [0.3611],\n",
       "        [0.2051],\n",
       "        [0.2089],\n",
       "        [0.4713],\n",
       "        [0.3147],\n",
       "        [0.0850],\n",
       "        [0.0432],\n",
       "        [0.1306],\n",
       "        [0.2328],\n",
       "        [0.2587],\n",
       "        [0.2623],\n",
       "        [0.2841],\n",
       "        [0.2818],\n",
       "        [0.2739],\n",
       "        [0.3661],\n",
       "        [0.0981],\n",
       "        [0.1959],\n",
       "        [0.1951],\n",
       "        [0.2854],\n",
       "        [0.2577],\n",
       "        [0.3431],\n",
       "        [0.1654],\n",
       "        [0.2700],\n",
       "        [0.4214],\n",
       "        [0.1422],\n",
       "        [0.1363],\n",
       "        [0.2132],\n",
       "        [0.1865],\n",
       "        [0.1960],\n",
       "        [0.0930],\n",
       "        [0.1040],\n",
       "        [0.2512],\n",
       "        [0.1593],\n",
       "        [0.0635],\n",
       "        [0.0891],\n",
       "        [0.2130],\n",
       "        [0.3493],\n",
       "        [0.2431],\n",
       "        [0.1376],\n",
       "        [0.1793],\n",
       "        [0.1252],\n",
       "        [0.4292],\n",
       "        [0.1648],\n",
       "        [0.1413],\n",
       "        [0.2266],\n",
       "        [0.0856],\n",
       "        [0.2838],\n",
       "        [0.4920],\n",
       "        [0.2019],\n",
       "        [0.2446],\n",
       "        [0.1541],\n",
       "        [0.0359],\n",
       "        [0.0989],\n",
       "        [0.3452],\n",
       "        [0.1213],\n",
       "        [0.2149],\n",
       "        [0.2886],\n",
       "        [0.1292],\n",
       "        [0.2404],\n",
       "        [0.4166],\n",
       "        [0.3502],\n",
       "        [0.1776],\n",
       "        [0.3721],\n",
       "        [0.2073],\n",
       "        [0.1965],\n",
       "        [0.0636],\n",
       "        [0.2390],\n",
       "        [0.1087],\n",
       "        [0.1574],\n",
       "        [0.3048],\n",
       "        [0.1765],\n",
       "        [0.3004],\n",
       "        [0.1931],\n",
       "        [0.2655],\n",
       "        [0.2172],\n",
       "        [0.2472],\n",
       "        [0.1248],\n",
       "        [0.2783],\n",
       "        [0.1670],\n",
       "        [0.2222],\n",
       "        [0.1949],\n",
       "        [0.3871],\n",
       "        [0.2186],\n",
       "        [0.1699],\n",
       "        [0.1965],\n",
       "        [0.2728],\n",
       "        [0.3543],\n",
       "        [0.0344],\n",
       "        [0.4852],\n",
       "        [0.0899],\n",
       "        [0.3245],\n",
       "        [0.1730],\n",
       "        [0.2998],\n",
       "        [0.3107],\n",
       "        [0.3440],\n",
       "        [0.1704],\n",
       "        [0.3295],\n",
       "        [0.3336],\n",
       "        [0.2806],\n",
       "        [0.0466],\n",
       "        [0.3402],\n",
       "        [0.1486],\n",
       "        [0.1840],\n",
       "        [0.2661],\n",
       "        [0.0271],\n",
       "        [0.1229],\n",
       "        [0.1931],\n",
       "        [0.3596],\n",
       "        [0.0945],\n",
       "        [0.3567],\n",
       "        [0.0323],\n",
       "        [0.3209],\n",
       "        [0.0980],\n",
       "        [0.1883],\n",
       "        [0.1330],\n",
       "        [0.5105],\n",
       "        [0.1211],\n",
       "        [0.4179],\n",
       "        [0.3153],\n",
       "        [0.2907],\n",
       "        [0.3180],\n",
       "        [0.3616],\n",
       "        [0.1541],\n",
       "        [0.2383],\n",
       "        [0.2677],\n",
       "        [0.3539],\n",
       "        [0.2755],\n",
       "        [0.2813],\n",
       "        [0.4025],\n",
       "        [0.3464],\n",
       "        [0.1654],\n",
       "        [0.3937],\n",
       "        [0.1690],\n",
       "        [0.1604],\n",
       "        [0.3513],\n",
       "        [0.3136],\n",
       "        [0.2212],\n",
       "        [0.2544],\n",
       "        [0.1301],\n",
       "        [0.1230],\n",
       "        [0.2296],\n",
       "        [0.3336],\n",
       "        [0.2103],\n",
       "        [0.1673],\n",
       "        [0.2621],\n",
       "        [0.2646],\n",
       "        [0.3844],\n",
       "        [0.1597],\n",
       "        [0.1404],\n",
       "        [0.1404],\n",
       "        [0.2254],\n",
       "        [0.0424],\n",
       "        [0.2315],\n",
       "        [0.2002],\n",
       "        [0.1498],\n",
       "        [0.1823],\n",
       "        [0.3568],\n",
       "        [0.2446],\n",
       "        [0.3614],\n",
       "        [0.2043],\n",
       "        [0.1222],\n",
       "        [0.4090],\n",
       "        [0.3032],\n",
       "        [0.2453],\n",
       "        [0.2145],\n",
       "        [0.1674],\n",
       "        [0.3654],\n",
       "        [0.1251],\n",
       "        [0.0566],\n",
       "        [0.2211],\n",
       "        [0.2472],\n",
       "        [0.2247],\n",
       "        [0.2172],\n",
       "        [0.2002],\n",
       "        [0.2797],\n",
       "        [0.2210],\n",
       "        [0.2271],\n",
       "        [0.4777],\n",
       "        [0.2652],\n",
       "        [0.3979]], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576a346b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0.0352],\n",
       "        [    0.1611],\n",
       "        [    0.0594],\n",
       "        [    0.1372],\n",
       "        [    0.0354],\n",
       "        [    0.2706],\n",
       "        [    0.0935],\n",
       "        [    0.0478],\n",
       "        [    0.0275],\n",
       "        [    0.0626],\n",
       "        [    0.1124],\n",
       "        [    0.1237],\n",
       "        [    0.1275],\n",
       "        [    0.0804],\n",
       "        [    0.1179],\n",
       "        [    0.0438],\n",
       "        [    0.1737],\n",
       "        [    0.1278],\n",
       "        [    0.0981],\n",
       "        [    0.1362],\n",
       "        [    0.1441],\n",
       "        [    0.0228],\n",
       "        [    0.0607],\n",
       "        [    0.0311],\n",
       "        [    0.1401],\n",
       "        [    0.1964],\n",
       "        [    0.1888],\n",
       "        [    0.0891],\n",
       "        [    0.0616],\n",
       "        [    0.1287],\n",
       "        [    0.0837],\n",
       "        [    0.0776],\n",
       "        [    0.0504],\n",
       "        [    0.1418],\n",
       "        [    0.1055],\n",
       "        [    0.0334],\n",
       "        [    0.0866],\n",
       "        [    0.1149],\n",
       "        [    0.3128],\n",
       "        [    0.2432],\n",
       "        [    0.0857],\n",
       "        [    0.1207],\n",
       "        [    0.1459],\n",
       "        [    0.0361],\n",
       "        [    0.0616],\n",
       "        [    0.0717],\n",
       "        [    0.1275],\n",
       "        [    0.1444],\n",
       "        [    0.0950],\n",
       "        [    0.0198],\n",
       "        [    0.0838],\n",
       "        [    0.6567],\n",
       "        [    0.3885],\n",
       "        [    0.0041],\n",
       "        [    0.1717],\n",
       "        [    0.0499],\n",
       "        [    0.1330],\n",
       "        [    0.1722],\n",
       "        [    0.0027],\n",
       "        [    0.0814],\n",
       "        [    0.0974],\n",
       "        [    0.2009],\n",
       "        [    0.0696],\n",
       "        [    0.0623],\n",
       "        [    0.0862],\n",
       "        [    0.0938],\n",
       "        [    0.2397],\n",
       "        [    0.0716],\n",
       "        [    0.0979],\n",
       "        [    0.0319],\n",
       "        [    0.0587],\n",
       "        [    0.0801],\n",
       "        [    0.0526],\n",
       "        [    0.0378],\n",
       "        [    0.1683],\n",
       "        [    0.0490],\n",
       "        [    0.0593],\n",
       "        [    0.0247],\n",
       "        [    0.2714],\n",
       "        [    0.0932],\n",
       "        [    0.2115],\n",
       "        [    0.0433],\n",
       "        [    0.0599],\n",
       "        [    0.1222],\n",
       "        [    0.0575],\n",
       "        [    0.0416],\n",
       "        [    0.0172],\n",
       "        [    0.2200],\n",
       "        [    0.0265],\n",
       "        [    0.0874],\n",
       "        [    0.0325],\n",
       "        [    0.1059],\n",
       "        [    0.1249],\n",
       "        [    0.0363],\n",
       "        [    0.1090],\n",
       "        [    0.0619],\n",
       "        [    0.1314],\n",
       "        [    0.1234],\n",
       "        [    0.2356],\n",
       "        [    0.0381],\n",
       "        [    0.0372],\n",
       "        [    0.0348],\n",
       "        [    0.1031],\n",
       "        [    0.0174],\n",
       "        [    0.0558],\n",
       "        [    0.0716],\n",
       "        [    0.0097],\n",
       "        [    0.1227],\n",
       "        [    0.0689],\n",
       "        [    0.0842],\n",
       "        [    0.1283],\n",
       "        [    0.0153],\n",
       "        [    0.0871],\n",
       "        [    0.0086],\n",
       "        [    0.0335],\n",
       "        [    0.0657],\n",
       "        [    0.0707],\n",
       "        [    0.0401],\n",
       "        [    0.1266],\n",
       "        [    0.0132],\n",
       "        [    0.1321],\n",
       "        [    0.0048],\n",
       "        [    0.0911],\n",
       "        [    0.1060],\n",
       "        [    0.1730],\n",
       "        [    0.0557],\n",
       "        [    0.0934],\n",
       "        [    0.1381],\n",
       "        [    0.0583],\n",
       "        [    0.0021],\n",
       "        [    0.0585],\n",
       "        [    0.0455],\n",
       "        [    0.0713],\n",
       "        [    0.2417],\n",
       "        [    0.0449],\n",
       "        [    0.0898],\n",
       "        [    0.0036],\n",
       "        [    0.0541],\n",
       "        [    0.1203],\n",
       "        [    0.0564],\n",
       "        [    0.0203],\n",
       "        [    0.2420],\n",
       "        [    0.0785],\n",
       "        [    0.0261],\n",
       "        [    0.0482],\n",
       "        [    0.1186],\n",
       "        [    0.0848],\n",
       "        [    0.0362],\n",
       "        [    0.1025],\n",
       "        [    0.4755],\n",
       "        [    0.1136],\n",
       "        [    0.2678],\n",
       "        [    0.3303],\n",
       "        [    0.0123],\n",
       "        [    0.1350],\n",
       "        [    0.2317],\n",
       "        [    0.0259],\n",
       "        [    0.0280],\n",
       "        [    0.1234],\n",
       "        [    0.0121],\n",
       "        [    0.2003],\n",
       "        [    0.0828],\n",
       "        [    0.0751],\n",
       "        [    0.0900],\n",
       "        [    0.0412],\n",
       "        [    0.0256],\n",
       "        [    0.0249],\n",
       "        [    0.1362],\n",
       "        [    0.0559],\n",
       "        [    0.0574],\n",
       "        [    0.0428],\n",
       "        [    0.1717],\n",
       "        [    0.0998],\n",
       "        [    0.1163],\n",
       "        [    0.0106],\n",
       "        [    0.0426],\n",
       "        [    0.1573],\n",
       "        [    0.0281],\n",
       "        [    0.0599],\n",
       "        [    0.0361],\n",
       "        [    0.0734],\n",
       "        [    0.0887],\n",
       "        [    0.0573],\n",
       "        [    0.1676],\n",
       "        [    0.1063],\n",
       "        [    0.0198],\n",
       "        [    0.2053],\n",
       "        [    0.0006],\n",
       "        [    0.2114],\n",
       "        [    0.1651],\n",
       "        [    0.0566],\n",
       "        [    0.0010],\n",
       "        [    0.0468],\n",
       "        [    0.0389],\n",
       "        [    0.0646],\n",
       "        [    0.1028],\n",
       "        [    0.0690],\n",
       "        [    0.0725],\n",
       "        [    0.1736],\n",
       "        [    0.2591],\n",
       "        [    0.1468],\n",
       "        [    0.0212],\n",
       "        [    0.0482],\n",
       "        [    0.1130],\n",
       "        [    0.1545],\n",
       "        [    0.0302],\n",
       "        [    0.0400],\n",
       "        [    0.1184],\n",
       "        [    0.0553],\n",
       "        [    0.1606],\n",
       "        [    0.0452],\n",
       "        [    0.1390],\n",
       "        [    0.0528],\n",
       "        [    0.0889],\n",
       "        [    0.0533],\n",
       "        [    0.1431],\n",
       "        [    0.0567],\n",
       "        [    0.0737],\n",
       "        [    0.1065],\n",
       "        [    0.0713],\n",
       "        [    0.1446],\n",
       "        [    0.0982],\n",
       "        [    0.0904],\n",
       "        [    0.1395],\n",
       "        [    0.1024],\n",
       "        [    0.2465],\n",
       "        [    0.0158],\n",
       "        [    0.2357],\n",
       "        [    0.0270],\n",
       "        [    0.1058],\n",
       "        [    0.1628],\n",
       "        [    0.2478],\n",
       "        [    0.1945],\n",
       "        [    0.0413],\n",
       "        [    0.1941],\n",
       "        [    0.0614],\n",
       "        [    0.0520],\n",
       "        [    0.0707],\n",
       "        [    0.0885],\n",
       "        [    0.0726],\n",
       "        [    0.1262],\n",
       "        [    0.0853],\n",
       "        [    0.1372],\n",
       "        [    0.1576],\n",
       "        [    0.1753],\n",
       "        [    0.1497],\n",
       "        [    0.1655],\n",
       "        [    0.0689],\n",
       "        [    0.1315],\n",
       "        [    0.1032],\n",
       "        [    0.0946],\n",
       "        [    0.1250],\n",
       "        [    0.0511],\n",
       "        [    0.2943],\n",
       "        [    0.0670],\n",
       "        [    0.0755],\n",
       "        [    0.1841],\n",
       "        [    0.0262],\n",
       "        [    0.2490],\n",
       "        [    0.2057],\n",
       "        [    0.1003],\n",
       "        [    0.0201],\n",
       "        [    0.1262],\n",
       "        [    0.1126],\n",
       "        [    0.0318],\n",
       "        [    0.0070],\n",
       "        [    0.2433],\n",
       "        [    0.1096],\n",
       "        [    0.0519],\n",
       "        [    0.1844],\n",
       "        [    0.0583],\n",
       "        [    0.0265],\n",
       "        [    0.0417],\n",
       "        [    0.0230],\n",
       "        [    0.0942],\n",
       "        [    0.0415],\n",
       "        [    0.2691],\n",
       "        [    0.0584],\n",
       "        [    0.0275],\n",
       "        [    0.0651],\n",
       "        [    0.0407],\n",
       "        [    0.0488],\n",
       "        [    0.0668],\n",
       "        [    0.2363],\n",
       "        [    0.1764],\n",
       "        [    0.1310],\n",
       "        [    0.0244],\n",
       "        [    0.1188],\n",
       "        [    0.0710],\n",
       "        [    0.0219],\n",
       "        [    0.1629],\n",
       "        [    0.0412],\n",
       "        [    0.1618],\n",
       "        [    0.0752],\n",
       "        [    0.0843],\n",
       "        [    0.2722],\n",
       "        [    0.1037],\n",
       "        [    0.0696],\n",
       "        [    0.2139],\n",
       "        [    0.0125],\n",
       "        [    0.0776],\n",
       "        [    0.0246],\n",
       "        [    0.0156],\n",
       "        [    0.0665],\n",
       "        [    0.0518],\n",
       "        [    0.1093],\n",
       "        [    0.1330],\n",
       "        [    0.0111],\n",
       "        [    0.0321],\n",
       "        [    0.1273],\n",
       "        [    0.0190],\n",
       "        [    0.0966],\n",
       "        [    0.0522],\n",
       "        [    0.2211],\n",
       "        [    0.1337],\n",
       "        [    0.0813],\n",
       "        [    0.0581],\n",
       "        [    0.0823],\n",
       "        [    0.0559],\n",
       "        [    0.0545],\n",
       "        [    0.1949],\n",
       "        [    0.2737],\n",
       "        [    0.0644],\n",
       "        [    0.0831],\n",
       "        [    0.0976],\n",
       "        [    0.0482],\n",
       "        [    0.0268],\n",
       "        [    0.1412],\n",
       "        [    0.0297],\n",
       "        [    0.0489],\n",
       "        [    0.1291],\n",
       "        [    0.1715],\n",
       "        [    0.1319],\n",
       "        [    0.2856],\n",
       "        [    0.1636],\n",
       "        [    0.0942],\n",
       "        [    0.1449],\n",
       "        [    0.0536],\n",
       "        [    0.0198],\n",
       "        [    0.1330],\n",
       "        [    0.2413],\n",
       "        [    0.1309],\n",
       "        [    0.0574],\n",
       "        [    0.0620],\n",
       "        [    0.0860],\n",
       "        [    0.1038],\n",
       "        [    0.1460],\n",
       "        [    0.0673],\n",
       "        [    0.1042],\n",
       "        [    0.0109],\n",
       "        [    0.1570],\n",
       "        [    0.0905],\n",
       "        [    0.2005],\n",
       "        [    0.1192],\n",
       "        [    0.0922],\n",
       "        [    0.1044],\n",
       "        [    0.0984],\n",
       "        [    0.0486],\n",
       "        [    0.2542],\n",
       "        [    0.0994],\n",
       "        [    0.0746],\n",
       "        [    0.0758],\n",
       "        [    0.0000],\n",
       "        [    0.1610],\n",
       "        [    0.0979],\n",
       "        [    0.1314],\n",
       "        [    0.0780],\n",
       "        [    0.1717],\n",
       "        [    0.2588],\n",
       "        [    0.0612],\n",
       "        [    0.0969],\n",
       "        [    0.1270],\n",
       "        [    0.0251],\n",
       "        [    0.1530],\n",
       "        [    0.1319],\n",
       "        [    0.0908],\n",
       "        [    0.1128],\n",
       "        [    0.1392],\n",
       "        [    0.0333],\n",
       "        [    0.1269],\n",
       "        [    0.0244],\n",
       "        [    0.0402],\n",
       "        [    0.0943],\n",
       "        [    0.0879],\n",
       "        [    0.0301],\n",
       "        [    0.0635],\n",
       "        [    0.1323],\n",
       "        [    0.0678],\n",
       "        [    0.0149],\n",
       "        [    0.0554],\n",
       "        [    0.0126],\n",
       "        [    0.1970],\n",
       "        [    0.1559],\n",
       "        [    0.0587],\n",
       "        [    0.1358],\n",
       "        [    0.1274],\n",
       "        [    0.1023],\n",
       "        [    0.0565],\n",
       "        [    0.1230],\n",
       "        [    0.0755],\n",
       "        [    0.0723],\n",
       "        [    0.0566],\n",
       "        [    0.1166],\n",
       "        [    0.1663],\n",
       "        [    0.1599],\n",
       "        [    0.3839],\n",
       "        [    0.0237],\n",
       "        [    0.1173],\n",
       "        [    0.0768],\n",
       "        [    0.0521],\n",
       "        [    0.1764],\n",
       "        [    0.0380],\n",
       "        [    0.0669],\n",
       "        [    0.1030],\n",
       "        [    0.0470],\n",
       "        [    0.1661],\n",
       "        [    0.0657],\n",
       "        [    0.2034],\n",
       "        [    0.0696],\n",
       "        [    0.0898],\n",
       "        [    0.1084],\n",
       "        [    0.2471],\n",
       "        [    0.0819],\n",
       "        [    0.2460],\n",
       "        [    0.0618],\n",
       "        [    0.0187],\n",
       "        [    0.0603],\n",
       "        [    0.1505],\n",
       "        [    0.0368],\n",
       "        [    0.1105],\n",
       "        [    0.0448],\n",
       "        [    0.2351],\n",
       "        [    0.1013],\n",
       "        [    0.1272],\n",
       "        [    0.1107],\n",
       "        [    0.1216],\n",
       "        [    0.0794],\n",
       "        [    0.1615],\n",
       "        [    0.0501],\n",
       "        [    0.1488],\n",
       "        [    0.1500],\n",
       "        [    0.0574],\n",
       "        [    0.0577],\n",
       "        [    0.1530],\n",
       "        [    0.1012],\n",
       "        [    0.0347],\n",
       "        [    0.0390],\n",
       "        [    0.1511],\n",
       "        [    0.0756],\n",
       "        [    0.1126],\n",
       "        [    0.0379],\n",
       "        [    0.1782],\n",
       "        [    0.2786],\n",
       "        [    0.1105],\n",
       "        [    0.1143],\n",
       "        [    0.1249],\n",
       "        [    0.0491],\n",
       "        [    0.1061],\n",
       "        [    0.0979],\n",
       "        [    0.0781],\n",
       "        [    0.0521],\n",
       "        [    0.1437],\n",
       "        [    0.0956],\n",
       "        [    0.1891],\n",
       "        [    0.0235],\n",
       "        [    0.1989],\n",
       "        [    0.0642],\n",
       "        [    0.1720],\n",
       "        [    0.0550],\n",
       "        [    0.0586],\n",
       "        [    0.1900],\n",
       "        [    0.0644],\n",
       "        [    0.3685],\n",
       "        [    0.1384],\n",
       "        [    0.0399],\n",
       "        [    0.4447],\n",
       "        [    0.0243],\n",
       "        [    0.1294],\n",
       "        [    0.0711],\n",
       "        [    0.0080],\n",
       "        [    0.1780],\n",
       "        [    0.1172],\n",
       "        [    0.0985],\n",
       "        [    0.0755],\n",
       "        [    0.0852],\n",
       "        [    0.2133],\n",
       "        [    0.1474],\n",
       "        [    0.1774],\n",
       "        [    0.0454],\n",
       "        [    0.1289],\n",
       "        [    0.0385],\n",
       "        [    0.1408],\n",
       "        [    0.0532],\n",
       "        [    0.0829],\n",
       "        [    0.1433],\n",
       "        [    0.1503],\n",
       "        [    0.0622],\n",
       "        [    0.1357],\n",
       "        [    0.0840],\n",
       "        [    0.1543],\n",
       "        [    0.0265],\n",
       "        [    0.0452],\n",
       "        [    0.2050],\n",
       "        [    0.2439],\n",
       "        [    0.1049],\n",
       "        [    0.2242],\n",
       "        [    0.0728],\n",
       "        [    0.0863],\n",
       "        [    0.0866],\n",
       "        [    0.0736],\n",
       "        [    0.0815],\n",
       "        [    0.0340],\n",
       "        [    0.0644],\n",
       "        [    0.0743],\n",
       "        [    0.0917],\n",
       "        [    0.2076],\n",
       "        [    0.0711],\n",
       "        [    0.2130],\n",
       "        [    0.0714],\n",
       "        [    0.0886],\n",
       "        [    0.0434],\n",
       "        [    0.0097],\n",
       "        [    0.0908],\n",
       "        [    0.0259],\n",
       "        [    0.1792],\n",
       "        [    0.0713],\n",
       "        [    0.1080],\n",
       "        [    0.1020],\n",
       "        [    0.0721],\n",
       "        [    0.0278],\n",
       "        [    0.0761],\n",
       "        [    0.3033],\n",
       "        [    0.0783],\n",
       "        [    0.2238],\n",
       "        [    0.1749],\n",
       "        [    0.0799],\n",
       "        [    0.0116],\n",
       "        [    0.0351],\n",
       "        [    0.0916],\n",
       "        [    0.0947],\n",
       "        [    0.1004],\n",
       "        [    0.1592],\n",
       "        [    0.1192],\n",
       "        [    0.0082],\n",
       "        [    0.0477],\n",
       "        [    0.1295],\n",
       "        [    0.0450],\n",
       "        [    0.0770],\n",
       "        [    0.1403],\n",
       "        [    0.0537],\n",
       "        [    0.0666],\n",
       "        [    0.0661],\n",
       "        [    0.0675],\n",
       "        [    0.1251],\n",
       "        [    0.1197],\n",
       "        [    0.0090],\n",
       "        [    0.0220],\n",
       "        [    0.0206],\n",
       "        [    0.0902],\n",
       "        [    0.3650],\n",
       "        [    0.0229],\n",
       "        [    0.0722],\n",
       "        [    0.0652],\n",
       "        [    0.0615],\n",
       "        [    0.0534],\n",
       "        [    0.1420],\n",
       "        [    0.0223],\n",
       "        [    0.0851],\n",
       "        [    0.0921],\n",
       "        [    0.2172],\n",
       "        [    0.1718],\n",
       "        [    0.1569],\n",
       "        [    0.3222],\n",
       "        [    0.2013],\n",
       "        [    0.0658],\n",
       "        [    0.0918],\n",
       "        [    0.0458],\n",
       "        [    0.0230],\n",
       "        [    0.2025],\n",
       "        [    0.0650],\n",
       "        [    0.0762],\n",
       "        [    0.0393],\n",
       "        [    0.0200],\n",
       "        [    0.1106],\n",
       "        [    0.1297],\n",
       "        [    0.0874],\n",
       "        [    0.1102],\n",
       "        [    0.1675],\n",
       "        [    0.0727],\n",
       "        [    0.1029],\n",
       "        [    0.0598],\n",
       "        [    0.1762],\n",
       "        [    0.0843],\n",
       "        [    0.1744],\n",
       "        [    0.1371],\n",
       "        [    0.1241],\n",
       "        [    0.0941],\n",
       "        [    0.1634],\n",
       "        [    0.0397],\n",
       "        [    0.0329],\n",
       "        [    0.0360],\n",
       "        [    0.2343],\n",
       "        [    0.0856],\n",
       "        [    0.1428],\n",
       "        [    0.1457],\n",
       "        [    0.1232],\n",
       "        [    0.0412],\n",
       "        [    0.1385],\n",
       "        [    0.2515],\n",
       "        [    0.0882],\n",
       "        [    0.0587],\n",
       "        [    0.0167],\n",
       "        [    0.0725],\n",
       "        [    0.0144],\n",
       "        [    0.0811],\n",
       "        [    0.0025],\n",
       "        [    0.0850],\n",
       "        [    0.0231],\n",
       "        [    0.0908],\n",
       "        [    0.0090],\n",
       "        [    0.0469],\n",
       "        [    0.0591],\n",
       "        [    0.0335],\n",
       "        [    0.1292],\n",
       "        [    0.0343],\n",
       "        [    0.1503],\n",
       "        [    0.1324],\n",
       "        [    0.1028],\n",
       "        [    0.1607],\n",
       "        [    0.0580],\n",
       "        [    0.0606],\n",
       "        [    0.0264],\n",
       "        [    0.1358],\n",
       "        [    0.0938],\n",
       "        [    0.0677],\n",
       "        [    0.0319],\n",
       "        [    0.1157],\n",
       "        [    0.0839],\n",
       "        [    0.0547],\n",
       "        [    0.0896],\n",
       "        [    0.0478],\n",
       "        [    0.1087],\n",
       "        [    0.2204],\n",
       "        [    0.0367],\n",
       "        [    0.1911],\n",
       "        [    0.0714],\n",
       "        [    0.1401],\n",
       "        [    0.0542],\n",
       "        [    0.1784],\n",
       "        [    0.0301],\n",
       "        [    0.1678],\n",
       "        [    0.0169],\n",
       "        [    0.1082],\n",
       "        [    0.1478],\n",
       "        [    0.1570],\n",
       "        [    0.0652],\n",
       "        [    0.0345],\n",
       "        [    0.2205],\n",
       "        [    0.0920],\n",
       "        [    0.1896],\n",
       "        [    0.0465],\n",
       "        [    0.0521],\n",
       "        [    0.2219],\n",
       "        [    0.0315],\n",
       "        [    0.0822],\n",
       "        [    0.1124],\n",
       "        [    0.0075],\n",
       "        [    0.1116],\n",
       "        [    0.0754],\n",
       "        [    0.0547],\n",
       "        [    0.0714],\n",
       "        [    0.1092],\n",
       "        [    0.0668],\n",
       "        [    0.0357],\n",
       "        [    0.0481],\n",
       "        [    0.1549],\n",
       "        [    0.0784],\n",
       "        [    0.0272],\n",
       "        [    0.1395],\n",
       "        [    0.1025],\n",
       "        [    0.0102],\n",
       "        [    0.0950],\n",
       "        [    0.0558],\n",
       "        [    0.0567],\n",
       "        [    0.1441],\n",
       "        [    0.1095],\n",
       "        [    0.1422],\n",
       "        [    0.1011],\n",
       "        [    0.0927],\n",
       "        [    0.0778],\n",
       "        [    0.1034],\n",
       "        [    0.0154],\n",
       "        [    0.0525],\n",
       "        [    0.1431],\n",
       "        [    0.0430],\n",
       "        [    0.0853],\n",
       "        [    0.0537],\n",
       "        [    0.0424],\n",
       "        [    0.1544],\n",
       "        [    0.1124],\n",
       "        [    0.2034],\n",
       "        [    0.3507],\n",
       "        [    0.0544],\n",
       "        [    0.0424],\n",
       "        [    0.0627],\n",
       "        [    0.0708],\n",
       "        [    0.0604],\n",
       "        [    0.0062],\n",
       "        [    0.1172],\n",
       "        [    0.0307],\n",
       "        [    0.0380],\n",
       "        [    0.0411],\n",
       "        [    0.0873],\n",
       "        [    0.1152],\n",
       "        [    0.0739],\n",
       "        [    0.0936],\n",
       "        [    0.0305],\n",
       "        [    0.1537],\n",
       "        [    0.1209],\n",
       "        [    0.0398],\n",
       "        [    0.1397],\n",
       "        [    0.1333],\n",
       "        [    0.1770],\n",
       "        [    0.0405],\n",
       "        [    0.1136],\n",
       "        [    0.1193],\n",
       "        [    0.0566],\n",
       "        [    0.1394],\n",
       "        [    0.1028],\n",
       "        [    0.2622],\n",
       "        [    0.1187],\n",
       "        [    0.0887],\n",
       "        [    0.0747],\n",
       "        [    0.0325],\n",
       "        [    0.1553],\n",
       "        [    0.0045],\n",
       "        [    0.0840],\n",
       "        [    0.1069],\n",
       "        [    0.0105],\n",
       "        [    0.1443],\n",
       "        [    0.0236],\n",
       "        [    0.0853],\n",
       "        [    0.0377],\n",
       "        [    0.0751],\n",
       "        [    0.0818],\n",
       "        [    0.0366],\n",
       "        [    0.2144],\n",
       "        [    0.0488],\n",
       "        [    0.0724],\n",
       "        [    0.0430],\n",
       "        [    0.0802],\n",
       "        [    0.0840],\n",
       "        [    0.0955],\n",
       "        [    0.2062],\n",
       "        [    0.0889],\n",
       "        [    0.0571],\n",
       "        [    0.1962],\n",
       "        [    0.1521],\n",
       "        [    0.0160],\n",
       "        [    0.0759],\n",
       "        [    0.1760],\n",
       "        [    0.0181],\n",
       "        [    0.0793],\n",
       "        [    0.0690],\n",
       "        [    0.0785],\n",
       "        [    0.0701],\n",
       "        [    0.1498],\n",
       "        [    0.1050],\n",
       "        [    0.1271],\n",
       "        [    0.1607],\n",
       "        [    0.0240],\n",
       "        [    0.2089],\n",
       "        [    0.0649],\n",
       "        [    0.0314],\n",
       "        [    0.0142],\n",
       "        [    0.0492],\n",
       "        [    0.0652],\n",
       "        [    0.0048],\n",
       "        [    0.0845],\n",
       "        [    0.0558],\n",
       "        [    0.0211],\n",
       "        [    0.0195],\n",
       "        [    0.3218],\n",
       "        [    0.1830],\n",
       "        [    0.0968],\n",
       "        [    0.1002],\n",
       "        [    0.0654],\n",
       "        [    0.1087],\n",
       "        [    0.0954],\n",
       "        [    0.0696],\n",
       "        [    0.0997],\n",
       "        [    0.1900],\n",
       "        [    0.1359],\n",
       "        [    0.0485],\n",
       "        [    0.0913],\n",
       "        [    0.0603],\n",
       "        [    0.0407],\n",
       "        [    0.1559],\n",
       "        [    0.1628],\n",
       "        [    0.0387],\n",
       "        [    0.0095],\n",
       "        [    0.0309],\n",
       "        [    0.0263],\n",
       "        [    0.1038],\n",
       "        [    0.1240],\n",
       "        [    0.0777],\n",
       "        [    0.1474],\n",
       "        [    0.0131],\n",
       "        [    0.0613],\n",
       "        [    0.0333],\n",
       "        [    0.1074],\n",
       "        [    0.1345],\n",
       "        [    0.3066],\n",
       "        [    0.1712],\n",
       "        [    0.0856],\n",
       "        [    0.0223],\n",
       "        [    0.0592],\n",
       "        [    0.1070],\n",
       "        [    0.0956],\n",
       "        [    0.1246],\n",
       "        [    0.1907],\n",
       "        [    0.0967],\n",
       "        [    0.0680],\n",
       "        [    0.3023],\n",
       "        [    0.1002],\n",
       "        [    0.0301],\n",
       "        [    0.0132],\n",
       "        [    0.0590],\n",
       "        [    0.0814],\n",
       "        [    0.1031],\n",
       "        [    0.1725],\n",
       "        [    0.1042],\n",
       "        [    0.0836],\n",
       "        [    0.1005],\n",
       "        [    0.1936],\n",
       "        [    0.0364],\n",
       "        [    0.0532],\n",
       "        [    0.0283],\n",
       "        [    0.1377],\n",
       "        [    0.1103],\n",
       "        [    0.0914],\n",
       "        [    0.1032],\n",
       "        [    0.2066],\n",
       "        [    0.1825],\n",
       "        [    0.0683],\n",
       "        [    0.0462],\n",
       "        [    0.1248],\n",
       "        [    0.0715],\n",
       "        [    0.1044],\n",
       "        [    0.0449],\n",
       "        [    0.0202],\n",
       "        [    0.1583],\n",
       "        [    0.0990],\n",
       "        [    0.0094],\n",
       "        [    0.0276],\n",
       "        [    0.0742],\n",
       "        [    0.1525],\n",
       "        [    0.1431],\n",
       "        [    0.0701],\n",
       "        [    0.0858],\n",
       "        [    0.0409],\n",
       "        [    0.1367],\n",
       "        [    0.0444],\n",
       "        [    0.0833],\n",
       "        [    0.2110],\n",
       "        [    0.0469],\n",
       "        [    0.1311],\n",
       "        [    0.2996],\n",
       "        [    0.0980],\n",
       "        [    0.0856],\n",
       "        [    0.0385],\n",
       "        [    0.0100],\n",
       "        [    0.0307],\n",
       "        [    0.1785],\n",
       "        [    0.0551],\n",
       "        [    0.0666],\n",
       "        [    0.1353],\n",
       "        [    0.0640],\n",
       "        [    0.1455],\n",
       "        [    0.1764],\n",
       "        [    0.1582],\n",
       "        [    0.0780],\n",
       "        [    0.1561],\n",
       "        [    0.1196],\n",
       "        [    0.1245],\n",
       "        [    0.0188],\n",
       "        [    0.1166],\n",
       "        [    0.0277],\n",
       "        [    0.0707],\n",
       "        [    0.0785],\n",
       "        [    0.0933],\n",
       "        [    0.1468],\n",
       "        [    0.0504],\n",
       "        [    0.0524],\n",
       "        [    0.0882],\n",
       "        [    0.1121],\n",
       "        [    0.0629],\n",
       "        [    0.1241],\n",
       "        [    0.0777],\n",
       "        [    0.0934],\n",
       "        [    0.0801],\n",
       "        [    0.0948],\n",
       "        [    0.1648],\n",
       "        [    0.1292],\n",
       "        [    0.0623],\n",
       "        [    0.1491],\n",
       "        [    0.0894],\n",
       "        [    0.0080],\n",
       "        [    0.2519],\n",
       "        [    0.0325],\n",
       "        [    0.2672],\n",
       "        [    0.1030],\n",
       "        [    0.1406],\n",
       "        [    0.1544],\n",
       "        [    0.2626],\n",
       "        [    0.0643],\n",
       "        [    0.1055],\n",
       "        [    0.1455],\n",
       "        [    0.1665],\n",
       "        [    0.0060],\n",
       "        [    0.1596],\n",
       "        [    0.0597],\n",
       "        [    0.0592],\n",
       "        [    0.1062],\n",
       "        [    0.0042],\n",
       "        [    0.0379],\n",
       "        [    0.1297],\n",
       "        [    0.1389],\n",
       "        [    0.0604],\n",
       "        [    0.2282],\n",
       "        [    0.0013],\n",
       "        [    0.0803],\n",
       "        [    0.0319],\n",
       "        [    0.0582],\n",
       "        [    0.0704],\n",
       "        [    0.1968],\n",
       "        [    0.0624],\n",
       "        [    0.1245],\n",
       "        [    0.2176],\n",
       "        [    0.1566],\n",
       "        [    0.0750],\n",
       "        [    0.2422],\n",
       "        [    0.0705],\n",
       "        [    0.0842],\n",
       "        [    0.1342],\n",
       "        [    0.1161],\n",
       "        [    0.0985],\n",
       "        [    0.1562],\n",
       "        [    0.2285],\n",
       "        [    0.1365],\n",
       "        [    0.0489],\n",
       "        [    0.2050],\n",
       "        [    0.0341],\n",
       "        [    0.0555],\n",
       "        [    0.1970],\n",
       "        [    0.0862],\n",
       "        [    0.1144],\n",
       "        [    0.0809],\n",
       "        [    0.0543],\n",
       "        [    0.0602],\n",
       "        [    0.0871],\n",
       "        [    0.1194],\n",
       "        [    0.0755],\n",
       "        [    0.0362],\n",
       "        [    0.0732],\n",
       "        [    0.1497],\n",
       "        [    0.1653],\n",
       "        [    0.0558],\n",
       "        [    0.0409],\n",
       "        [    0.0449],\n",
       "        [    0.0774],\n",
       "        [    0.0095],\n",
       "        [    0.1204],\n",
       "        [    0.1161],\n",
       "        [    0.1356],\n",
       "        [    0.0781],\n",
       "        [    0.1350],\n",
       "        [    0.0618],\n",
       "        [    0.1865],\n",
       "        [    0.0706],\n",
       "        [    0.0317],\n",
       "        [    0.2076],\n",
       "        [    0.1763],\n",
       "        [    0.0832],\n",
       "        [    0.0624],\n",
       "        [    0.0521],\n",
       "        [    0.3038],\n",
       "        [    0.0784],\n",
       "        [    0.0154],\n",
       "        [    0.1249],\n",
       "        [    0.1049],\n",
       "        [    0.0642],\n",
       "        [    0.1548],\n",
       "        [    0.0993],\n",
       "        [    0.1538],\n",
       "        [    0.1977],\n",
       "        [    0.0466],\n",
       "        [    0.1698],\n",
       "        [    0.0839],\n",
       "        [    0.1516]], grad_fn=<VarBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = out.var(dim=1, keepdim=True, unbiased = False)\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b97c0104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000],\n",
       "        [1.0000]], grad_fn=<VarBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normed = ((out - mean) / torch.sqrt(var))\n",
    "normed.var(dim=-1, keepdim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c464be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96a6ab05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=1, keepdim = True)\n",
    "        var = x.var(dim=-1, keepdim = True, unbiased = False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ecae9f",
   "metadata": {},
   "source": [
    "#### **Biased variance**\n",
    "\n",
    "- In the variance calculation above, setting `unbiased=False` means using the formula `mean_formula` to compute the variance where n is the sample size (here, the number of features or columns); this formula does not include Bessel's correction (which uses `n-1` in the denominator), thus providing a biased estimate of the variance\n",
    "\n",
    "- For LLMs, where the embedding dimension `n` is very large, the difference between using n and `n-1` is negligible\n",
    "\n",
    "- However, GPT-2 was trained with a biased variance in the normalization layers, which is why we also adopted this setting for compatibility reasons with the pretrained weights that we will load in later chapters\n",
    "\n",
    "- Let's now try out `LayerNorm` in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f3a7ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f5300f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, keepdim=True, unbiased = False)\n",
    "\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025b8e09",
   "metadata": {},
   "source": [
    "## **4.3 Implementing a fedd forward network with GELU activations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ca8d060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1a8fda43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "# Some sample data\n",
    "x = torch.linspace(-2, 2, 10000)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78dda57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg['emb_dim'], 4 * cfg['emb_dim']),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg['emb_dim'], cfg['emb_dim']),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1298273e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "print(GPT_CONFIG_124M[\"emb_dim\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc8b3ad",
   "metadata": {},
   "source": [
    "![](https://camo.githubusercontent.com/5b893aa5271d2f828f427f33208e4eadc1429c26c9bf4d2a1760b1384b1eeebc/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830345f636f6d707265737365642f30392e776562703f3132)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3e8201d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "\n",
    "# input shape: [batch_size, num_token, emb_size]\n",
    "x = torch.rand(2, 3, 768) \n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f27506",
   "metadata": {},
   "source": [
    "## **4.4 Adding shortcut Connections**\n",
    "\n",
    "![](https://camo.githubusercontent.com/b1cb95fee4a11c35cb6ce14a2399ac09875c45dc92d344713a80913e05e04c20/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830345f636f6d707265737365642f31322e776562703f313233)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "09a4edfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # Compute the output of the current layer\n",
    "            layer_output = layer(x)\n",
    "            # Check if shortcut can be applied\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n",
    "    \n",
    "def print_gradients(model, x):\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    # Calculate loss based on how close the target\n",
    "    # and output are\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    \n",
    "    # Backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # Print the mean absolute gradient of the weights\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a1ccbaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152041071094573\n",
      "layers.3.0.weight has gradient mean of 0.0013988735154271126\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]  \n",
    "\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")\n",
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "73df7ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169791162014008\n",
      "layers.1.0.weight has gradient mean of 0.20694106817245483\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258540630340576\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19450c18",
   "metadata": {},
   "source": [
    "## **4.5 Connecting attention and linear layers in a transformer block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5daba2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import MultiHeadAttention\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x \n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d5658b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "x = torch.rand(2, 4, 768)\n",
    "\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "\n",
    "output = block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "82b5a835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 768])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1c5dba72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 768])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3a1741",
   "metadata": {},
   "source": [
    "## **4.6 Coding the GPT Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d71da6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg['n_layers'])]\n",
    "            )\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg['emb_dim'], cfg[\"vocab_size\"], bias = False\n",
    "        )\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db03f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dee831ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50257, 768])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tok_emb.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fc3a8212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50257, 768])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.out_head.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6cad1117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Every effort moves you', 'Every day holds a', torch.Size([2, 4]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt1, txt2, batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "88c7f1e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 50257])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "732def81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fd93aaa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163009536"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "646be0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124,412,160\n"
     ]
    }
   ],
   "source": [
    "print(f\"{total_params - model.out_head.weight.numel():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae7de3b",
   "metadata": {},
   "source": [
    "### **GPT2-medium**\n",
    "```python\n",
    "GPT_CONFIG_345M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 1024,         # Embedding dimension\n",
    "    \"n_heads\": 16,          # Number of attention heads\n",
    "    \"n_layers\": 24,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b9c2a2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_345M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 1024,         # Embedding dimension\n",
    "    \"n_heads\": 16,          # Number of attention heads\n",
    "    \"n_layers\": 24,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9b8fde81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.4343,  0.2477,  0.8775,  ..., -0.1311, -0.2197,  0.3829],\n",
      "         [-0.2466, -0.7258, -0.3185,  ..., -0.4779, -0.4657, -0.7264],\n",
      "         [ 0.1318, -0.0210, -0.0625,  ...,  0.4281,  0.3424,  0.6749],\n",
      "         [-0.4247,  0.5233, -0.7132,  ...,  0.2583,  0.4504, -0.3900]],\n",
      "\n",
      "        [[ 0.6725, -0.2065,  1.0774,  ...,  0.2958, -0.2406,  0.8569],\n",
      "         [-0.2785, -0.2577, -0.4782,  ..., -0.2510,  0.1969, -0.5817],\n",
      "         [-0.2633, -0.1364, -0.0722,  ...,  0.1630, -0.1352,  0.1163],\n",
      "         [-0.2686,  0.6985, -0.7689,  ..., -0.2730,  0.2329, -0.5725]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_345M)\n",
    "\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "72abe7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 406,212,608\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4ce72959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 1024])\n",
      "Output layer shape: torch.Size([50257, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bd2b97ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 354,749,440\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 =  total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3d3f6593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 1549.58 MB\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total size in bytes (assuming float32, 4 bytes per parameter)\n",
    "total_size_bytes = total_params * 4\n",
    "\n",
    "# Convert to megabytes\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959dd400",
   "metadata": {},
   "source": [
    "### **GPT2-Large**\n",
    "```python\n",
    "GPT_CONFIG_762M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 1280,         # Embedding dimension\n",
    "    \"n_heads\": 20,          # Number of attention heads\n",
    "    \"n_layers\": 36,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9bbda63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_762M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 1280,         # Embedding dimension\n",
    "    \"n_heads\": 20,          # Number of attention heads\n",
    "    \"n_layers\": 36,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2fea5cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-0.6956,  0.1888,  0.8320,  ...,  0.7104,  0.8995, -0.2434],\n",
      "         [ 0.1786,  0.3723, -0.9961,  ..., -0.3018, -0.1734, -0.2609],\n",
      "         [ 0.1196, -0.2275,  0.1111,  ..., -0.9261, -0.8215,  0.6183],\n",
      "         [ 0.5825, -0.4191, -0.1041,  ...,  0.3299, -0.1552, -0.0180]],\n",
      "\n",
      "        [[-0.6264,  0.5989,  0.3776,  ...,  0.7163,  0.6254, -0.6554],\n",
      "         [ 0.5331,  0.0969, -0.2687,  ...,  0.0606,  0.3480,  0.1230],\n",
      "         [ 0.1318, -0.7127, -0.3043,  ..., -1.0498, -0.5861,  0.4727],\n",
      "         [ 0.0928, -0.1479,  0.1218,  ...,  0.0923, -0.5960,  0.2297]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_762M)\n",
    "\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "32dcae93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 838,220,800\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "98f944da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 1280])\n",
      "Output layer shape: torch.Size([50257, 1280])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "26bb7c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 773,891,840\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 =  total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "eedd7e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 3197.56 MB\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total size in bytes (assuming float32, 4 bytes per parameter)\n",
    "total_size_bytes = total_params * 4\n",
    "\n",
    "# Convert to megabytes\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae9504a",
   "metadata": {},
   "source": [
    "### **GPT2-XL**\n",
    "```python\n",
    "GPT_CONFIG_1542M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 1600,         # Embedding dimension\n",
    "    \"n_heads\": 25,          # Number of attention heads\n",
    "    \"n_layers\": 48,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bd31ef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_1542M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 1600,         # Embedding dimension\n",
    "    \"n_heads\": 25,          # Number of attention heads\n",
    "    \"n_layers\": 48,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "505faadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.4798,  0.3954,  0.0103,  ...,  0.0656, -0.0852, -0.3630],\n",
      "         [ 0.2904,  0.3560, -0.6912,  ..., -0.3461,  0.1303, -0.2102],\n",
      "         [ 0.0062, -0.1553,  0.4176,  ...,  0.1434,  0.5843,  0.1511],\n",
      "         [-0.9854, -0.7785,  0.3243,  ...,  0.1453, -0.6150,  0.5780]],\n",
      "\n",
      "        [[ 0.3600,  0.5446, -0.0597,  ..., -0.3957,  0.4731, -0.4495],\n",
      "         [ 0.4760, -0.1653, -0.3279,  ...,  0.0303, -0.6854, -0.6495],\n",
      "         [-0.0708, -0.2802,  0.3998,  ..., -0.5398,  0.1178,  0.2946],\n",
      "         [-0.9527, -0.2805,  0.0268,  ...,  1.0823, -0.0229,  1.0347]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_1542M)\n",
    "\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b63b7398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 1,637,792,000\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b12146a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 1600])\n",
      "Output layer shape: torch.Size([50257, 1600])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a5b819b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 1,557,380,800\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 =  total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b62182f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 6247.68 MB\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total size in bytes (assuming float32, 4 bytes per parameter)\n",
    "total_size_bytes = total_params * 4\n",
    "\n",
    "# Convert to megabytes\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad37c180",
   "metadata": {},
   "source": [
    "## **4.6 Generating Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1c5df06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(torch.tensor([10,1,2,3,4,5,6,1,2,3,2,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "80f74a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        idx_next = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "\n",
    "        idx = torch.cat((idx, idx_next), dim=-1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fedc1459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716, 2746]\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am model\"\n",
    "\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bc6c6093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_tensor.shape: torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0) # Our model want batch \n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e63ae388",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = generate_text_simple(\n",
    "    model, \n",
    "    idx = encoded_tensor, \n",
    "    max_new_tokens = 6, \n",
    "    context_size = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cfd3b43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15496,    11,   314,   716,  2746, 49760, 27385, 23818, 28193,  1440,\n",
       "          4549]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bca17f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496, 11, 314, 716, 2746, 49760, 27385, 23818, 28193, 1440, 4549]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.squeeze(0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "54bbd0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, I am model composinglys cumulative Zar fouricago'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out.squeeze(0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5465fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
